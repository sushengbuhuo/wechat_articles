<p data-nodeid="62949">我们在前面的部分里，已经学习完了 NumPy 的基础操作和用法。我们在前文中提到过，NumPy 是数值分析的神器，现在神器已经学得差不多了。现在我们来学习数值分析中最常见的一类分析方式：回归分析。</p>
<h3 data-nodeid="62950">什么是回归分析</h3>
<p data-nodeid="62951">回归分析，本质是在寻找不同变量之间的关系。</p>
<p data-nodeid="62952">这里的变量和 Python 的变量不太一样，这里的变量指的是统计学中的变量，统计学的变量指的是说明事物某种特征的量，变量往往会根据多次的观测，有多个值。</p>
<p data-nodeid="62953">举个例子，我们希望统计房屋的地段、面积、楼层和房价的关系。这里面的地段、面积、楼层和房价都是变量。回归分析是怎么确定他们之间的关系的呢？答案是：通过大量的观测。</p>
<p data-nodeid="65106" class="">观测（也可以说是观察）也是一个统计学的说法，整体来说指的就是通过搜集现有的数据。比如我们搜集不同地区，不同楼层的房子的数据，整理成一个表格。每个变量都是一列：地段、面积、楼层、房价。我们希望找地段、面积、楼层和单价的关系，简单来说就是找到一个这样的函数：</p>

<p data-nodeid="65964" class=""><img src="https://s0.lgstatic.com/i/image6/M01/44/1F/Cgp9HWC90yGAFgWwAAASh_p7WuY552.png" alt="Drawing 1.png" data-nodeid="65967"></p>

<p data-nodeid="62956">f 是什么样形式的函数，原先我们是不知道的，我们有的只是大量的房屋数据。<strong data-nodeid="63154">所谓回归分析，就是通过大量已有的数据，来拟合出对应关系 f。同样对于新的数据，只要我们知道了房屋的地段、面积和楼层，就能够估算出其房价。</strong></p>
<p data-nodeid="62957">在上面的变量中，地段、面积和楼层称之为自变量，单价称为因变量。</p>
<p data-nodeid="62958">回归分析在现实中有非常多的应用。我们只需要把现实问题用自变量、因变量的思路去建模，就能够使用回归分析找到自变量和因变量的关系，以及用自变量与预测未知的因变量。</p>
<p data-nodeid="62959">比如我们有员工的基本信息，我们就能通过回归分析来分析哪些变量（年龄、性别、籍贯、学历）和员工收入的函数关系。这样之后我们只要知道了员工的这些基本信息，就能够预测出员工的收入。</p>
<h3 data-nodeid="62960">线性回归</h3>
<h4 data-nodeid="62961">定义</h4>
<p data-nodeid="62962">回归分析有非常多种，其中最基础、使用最广的就是线性回归。线性回归也是相对来说最容易理解的回归分析方法。</p>
<p data-nodeid="62963">线性回归，顾名思义，就是说自变量和因变量的函数关系是线性关系。以我们上面说的房屋价格问题为例。如果用线性回归来建模这个问题，那这个问题就可以转换为以下公式：</p>
<p data-nodeid="66828" class=""><img src="https://s0.lgstatic.com/i/image6/M01/44/27/CioPOWC90zKAZ7-6AAAiODZlGZA693.png" alt="Drawing 3.png" data-nodeid="66831"></p>


<p data-nodeid="62966">为了简化表示，我们用假设 x0 = 地段，x1 = 面积，x2 =  楼层，y = 单价。上述式子可以化简为这样：</p>
<p data-nodeid="67678" class=""><img src="https://s0.lgstatic.com/i/image6/M01/44/27/CioPOWC90zqAO9MdAAAZlaldQYQ341.png" alt="Drawing 5.png" data-nodeid="67681"></p>


<p data-nodeid="62969">对于线性回归问题，本质就是要通过已知的一批 (y, x0, x1, x2) 的观测记录，来求得线性函数的四个系数 a、b、c、d 的值。</p>
<h4 data-nodeid="62970">误差衡量</h4>
<p data-nodeid="73724" class="">通过已有的观测来估算 a、b、c、d 往往都不是精准的，会存在一定的误差。那我们怎么衡量我们找到的系数是对的呢？我们首先需要能够描述一组特定的系数，比如：a',b',c',d' 到底有多贴近真实的 abcd。我们把参数等于 a',b',c',d' 的函数 f 记做 f'。</p>







<p data-nodeid="77960" class="">然后假设我们有 m 条观测记录，每条记录包含 y,x0,x1,x2 四个值。a',b',c',d' 这组参数的误差就等于：</p>




<p data-nodeid="74590" class=""><img src="https://s0.lgstatic.com/i/image6/M00/44/28/CioPOWC909mAWicfAAARyC_qDSE409.png" alt="Drawing 10.png" data-nodeid="74593"></p>


<p data-nodeid="82236" class="">这个公式也被称为 SSR（Sum of Square Residuals），残差平方和。指的是我们针对每一条记录，用 a',b',c',d' 系数确定的函数 f', 结合 x0，x1，x2 计算出一个 y'。y' 是我们估算系数计算出的估计值，而我们记录中的 y 则是实际值。用估算值减去实际值就得到了这条记录的误差。然后，我们将每条记录的误差求平方并加总起来，就得到了参数 a',b',c',d' 的总误差。</p>





<p data-nodeid="83094" class="">所以，线性回归的问题进一步转化为，<strong data-nodeid="83100">找到一组特定的 a,b,c,d，使得总误差函数的结果最小。</strong> 一般情况下比较常见的方法有梯度下降，由于 Python 已经提供了现成的拟合能力，并不需要我们手工计算，所以如何计算的过程在此就不展开。</p>

<h4 data-nodeid="62977">线性回归的几何意义</h4>
<p data-nodeid="62978">在这一节，我们通过一个直观的例子来说明线性回归希望解决的问题。假设我们只有一个自变量和一个因变量。那么我们的函数形式就是一个类似直线方程的形式：</p>
<p data-nodeid="83937" class=""><img src="https://s0.lgstatic.com/i/image6/M00/44/20/Cgp9HWC91AmAMR7GAAAP9kQi9TM833.png" alt="Drawing 14.png" data-nodeid="83940"></p>


<p data-nodeid="62981">假设我们有收集到 5 组（x,y）的观测记录，分别如下：</p>
<table data-nodeid="62983">
<thead data-nodeid="62984">
<tr data-nodeid="62985">
<th data-nodeid="62987">x</th>
<th data-nodeid="62988">y</th>
</tr>
</thead>
<tbody data-nodeid="62991">
<tr data-nodeid="62992">
<td data-nodeid="62993">1.0</td>
<td data-nodeid="62994">35.1</td>
</tr>
<tr data-nodeid="62995">
<td data-nodeid="62996">2.0</td>
<td data-nodeid="62997">42.9</td>
</tr>
<tr data-nodeid="62998">
<td data-nodeid="62999">3.0</td>
<td data-nodeid="63000">39.7</td>
</tr>
<tr data-nodeid="63001">
<td data-nodeid="63002">4.0</td>
<td data-nodeid="63003">41.5</td>
</tr>
<tr data-nodeid="63004">
<td data-nodeid="63005">5.0</td>
<td data-nodeid="63006">49.3</td>
</tr>
</tbody>
</table>
<p data-nodeid="63007">那我们希望用线性回归解决的问题就是通过这五组记录，估算出方程中的 k 和 b。</p>
<p data-nodeid="63008">首先我们把每组 (x,y) 都当成平面上的一个点。那这五个点画出来就是像下面这样的一张图：</p>
<p data-nodeid="84777" class=""><img src="https://s0.lgstatic.com/i/image6/M00/44/28/CioPOWC91BSAdoU1AABtdkSC9wg911.png" alt="Drawing 15.png" data-nodeid="84780"></p>

<p data-nodeid="63010">我们要寻找的 k 和 b，本质上就是希望寻找到一条直线，尽可能地靠近这些点。比如可以这样画：</p>
<p data-nodeid="85621" class=""><img src="https://s0.lgstatic.com/i/image6/M00/44/28/CioPOWC91BqAZhbSAACqo_DPJJ4908.png" alt="Drawing 16.png" data-nodeid="85624"></p>

<p data-nodeid="63012">也可以这样画：</p>
<p data-nodeid="86469" class=""><img src="https://s0.lgstatic.com/i/image6/M00/44/28/CioPOWC91CGAe7YUAACz4WHFNjI434.png" alt="Drawing 17.png" data-nodeid="86472"></p>

<p data-nodeid="63014">那我们到底怎么衡量哪条直线是最好的呢？一般来说，常见的方法是统计每个点到直线的距离，再将这些距离加起来。哪条线的距离和最小，说明哪条线就是最优的，具体就是下图中的五条小竖线：</p>
<p data-nodeid="87321" class=""><img src="https://s0.lgstatic.com/i/image6/M00/44/20/Cgp9HWC91C6Afl9gAAC09qVeM_8509.png" alt="Drawing 18.png" data-nodeid="87324"></p>

<p data-nodeid="63016" class="">线上的点和我们观测的点，x 是一样的。相减等于 0，所以我们只需要比较两个 y 的差即可。也就是说，对于特定的 k', b'，小竖线的长度总和可以表示为如下公式：</p>
<p data-nodeid="88177" class=""><img src="https://s0.lgstatic.com/i/image6/M00/44/29/CioPOWC91EyAaTLWAAAcG-ceX5U274.png" alt="Drawing 20.png" data-nodeid="88180"></p>


<p data-nodeid="63019" class="">我们让 i 从 1 到 5 循环，来分别计算每个 x 对应的 y 和 y' 的距离并加总。但这样会有一个问题：有的 y 比 y' 大，比如第二个点，有的 y 却比 y' 小，比如第三个点。但我们感兴趣的只是距离，这样就会因为符号问题导致计算出现误差。解决的办法也很简单，我们直接把两个 y 相减加上一个平方，这样就不会有符号问题了。</p>
<p data-nodeid="63020">所以公式修改如下：</p>
<p data-nodeid="89013" class=""><img src="https://s0.lgstatic.com/i/image6/M00/44/20/Cgp9HWC91GeAEk3nAAARFiC0Hp0792.png" alt="Drawing 22.png" data-nodeid="89016"></p>


<p data-nodeid="63023">上面的公式看着是不是很眼熟？没错，这就是前文所讲的残差平方和的公式。讲到这里，相信你对于线性回归的误差衡量有了一个更加直观的感受。</p>
<h3 data-nodeid="63024">用 Python 实现线性回归</h3>
<p data-nodeid="63025">上面我介绍了什么是线性回归，线性回归能解决什么样的问题，以及如何衡量回归分析的结果好不好。相信你一定已经迫不及待地想要学习到底怎么做回归分析，怎么从观测记录中计算线性函数的系数。</p>
<p data-nodeid="63026">本章我们以一个具体的例子来介绍如何使用 Python 完成回归分析的计算。</p>
<p data-nodeid="63027">回归分析的数学原理，简单地来说就是通过对误差函数求导的形式来做梯度下降，不断迭代出最佳的系数（比如一元线性回归的 k 和 b）。这里涉及比较多数学原理，我就不再展开。我们直接使用 Python 提供的方案即可完成。</p>
<h4 data-nodeid="63028">环境准备</h4>
<p data-nodeid="63029">为了实现回归分析，我们需要安装一个新的工具包：scikit-learn 来和 NumPy 配合完成。</p>
<p data-nodeid="63030">我们按照之前的方式，打开开始菜单→ Anaconda3 → Anaconda3 Prompt ，输入 conda install scikit-learn 之后回车，出现以下界面：</p>
<p data-nodeid="89837" class=""><img src="https://s0.lgstatic.com/i/image6/M00/44/29/CioPOWC91HWAIs99AAEC1AP9Gog021.png" alt="Drawing 23.png" data-nodeid="89840"></p>

<p data-nodeid="63032">输入 y 回车，完成安装。</p>
<h4 data-nodeid="63033">任务描述</h4>
<p data-nodeid="63034">众所周知，房子的价格很大程度取决于地段。我们观测了十套房子的单价以及它们距离市中心的距离。如下所示：</p>
<table data-nodeid="63036">
<thead data-nodeid="63037">
<tr data-nodeid="63038">
<th data-nodeid="63040">距离市中心的距离（公里）</th>
<th data-nodeid="63041">单价（万元）</th>
</tr>
</thead>
<tbody data-nodeid="63044">
<tr data-nodeid="63045">
<td data-nodeid="63046">1.0</td>
<td data-nodeid="63047">10.14</td>
</tr>
<tr data-nodeid="63048">
<td data-nodeid="63049">2.0</td>
<td data-nodeid="63050">9.89</td>
</tr>
<tr data-nodeid="63051">
<td data-nodeid="63052">3.0</td>
<td data-nodeid="63053">8.41</td>
</tr>
<tr data-nodeid="63054">
<td data-nodeid="63055">4.0</td>
<td data-nodeid="63056">9.61</td>
</tr>
<tr data-nodeid="63057">
<td data-nodeid="63058">5.0</td>
<td data-nodeid="63059">7.95</td>
</tr>
<tr data-nodeid="63060">
<td data-nodeid="63061">6.0</td>
<td data-nodeid="63062">7.46</td>
</tr>
<tr data-nodeid="63063">
<td data-nodeid="63064">7.0</td>
<td data-nodeid="63065">6.62</td>
</tr>
<tr data-nodeid="63066">
<td data-nodeid="63067">8.0</td>
<td data-nodeid="63068">5.23</td>
</tr>
<tr data-nodeid="63069">
<td data-nodeid="63070">9.0</td>
<td data-nodeid="63071">4.70</td>
</tr>
<tr data-nodeid="63072">
<td data-nodeid="63073">10.0</td>
<td data-nodeid="63074">4.77</td>
</tr>
</tbody>
</table>
<p data-nodeid="63075">假设距离市中心的距离和单价存在线性关系。尝试基于上述数据进行回归分析，求得线性函数的系数，并预测距离市区 15 公里的房屋单价。</p>
<h4 data-nodeid="63076">问题分析</h4>
<p data-nodeid="63077">我们希望通过距离来预测单价，所以这是一个典型的一元线性回归问题。距离是自变量，记做 x。单价是因变量，记做 y。所以我们要求的函数就是：</p>
<p data-nodeid="90665" class=""><img src="https://s0.lgstatic.com/i/image6/M00/44/29/CioPOWC91IGAJdKkAAAMBrlsUXI865.png" alt="Drawing 25.png" data-nodeid="90668"></p>


<p data-nodeid="63080">k 和 b 就是线性函数的系数，只要求得 k 和 b，我们只要把 x 带入 15.0，就能得到距离市区 15 公里的地方的单价的预测值。</p>
<h4 data-nodeid="63081">任务实现</h4>
<p data-nodeid="63082">（1）载入数据。我们把已经列好的数组分别用两个多维数组来存储。每个数组都是 1 维即可。</p>
<pre class="lang-python" data-nodeid="63083"><code data-language="python">x&nbsp;=&nbsp;np.array([<span class="hljs-number">1.0</span>,&nbsp;<span class="hljs-number">2.0</span>,&nbsp;<span class="hljs-number">3.0</span>,&nbsp;<span class="hljs-number">4.0</span>,&nbsp;<span class="hljs-number">5.0</span>&nbsp;,<span class="hljs-number">6.0</span>,<span class="hljs-number">7.0</span>,<span class="hljs-number">8.0</span>,<span class="hljs-number">9.0</span>,<span class="hljs-number">10</span>])
y&nbsp;=&nbsp;np.array([<span class="hljs-number">10.14</span>&nbsp;,&nbsp;&nbsp;<span class="hljs-number">9.89</span>&nbsp;,&nbsp;&nbsp;<span class="hljs-number">8.41</span>,&nbsp;&nbsp;<span class="hljs-number">9.61</span>,&nbsp;&nbsp;<span class="hljs-number">7.95</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-number">7.46</span>,&nbsp;&nbsp;<span class="hljs-number">6.62</span>,&nbsp;&nbsp;<span class="hljs-number">5.23</span>,&nbsp;&nbsp;<span class="hljs-number">4.70</span>,&nbsp;&nbsp;<span class="hljs-number">4.77</span>])
</code></pre>
<p data-nodeid="63084">（2）实现画图函数。为了方便分析，我们先实现两个工具函数，一个用来画点，一个用来画点+线。由于我们还没有学习 matplotlib，所以代码的含义可以先不用关心，理解函数怎么使用即可。</p>
<pre class="lang-python" data-nodeid="63085"><code data-language="python"><span class="hljs-keyword">import</span>&nbsp;matplotlib&nbsp;
<span class="hljs-keyword">import</span>&nbsp;matplotlib.pyplot&nbsp;<span class="hljs-keyword">as</span>&nbsp;plt
<span class="hljs-comment">#&nbsp;将数组&nbsp;x和数组&nbsp;y&nbsp;一一对应，以逐个点的形式画在坐标系上</span>
<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">draw</span>(<span class="hljs-params">x,y</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;matplotlib.style.use(<span class="hljs-string">"ggplot"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;plt.scatter(x,&nbsp;y,c=<span class="hljs-string">'b'</span>&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;plt.show()
<span class="hljs-comment">#&nbsp;将数组&nbsp;x和数组&nbsp;y&nbsp;一一对应，以逐个点的形式画在坐标系上</span>
<span class="hljs-comment">#&nbsp;然后将数组&nbsp;x&nbsp;和数组&nbsp;y1&nbsp;一一对应，以直线的形式画在坐标系上</span>
<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">draw_point_line</span>(<span class="hljs-params">x,y,y1</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;matplotlib.style.use(<span class="hljs-string">"ggplot"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;plt.scatter(x,&nbsp;y,c=<span class="hljs-string">'b'</span>&nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;plt.plot(x,&nbsp;y1,&nbsp;c=<span class="hljs-string">'r'</span>)
&nbsp;&nbsp;&nbsp;&nbsp;plt.show()
</code></pre>
<p data-nodeid="63086">（3）查看观测数据的分布。我们调用 draw 函数来将我们的观测值画出来，看看是否有具备线性关系。</p>
<pre class="lang-python" data-nodeid="63087"><code data-language="python">draw(x,y)
</code></pre>
<p data-nodeid="91493" class="">输出如下：<br>
<img src="https://s0.lgstatic.com/i/image6/M00/44/29/CioPOWC91IqAC9PjAABahHLjoZw454.png" alt="Drawing 26.png" data-nodeid="91498"></p>

<p data-nodeid="63089">可以看到，确实如任务描述章节所述，随着距离市区的公里数越多（x轴）房屋的单价就越低（y 轴）。而且看起来可以找到一条直线尽可能地靠近目前所有的点。</p>
<p data-nodeid="63090">（4）初始化线性回归器对象。线性回归器（LinearRegression）是 scikit-learn 工具包中的一个类，通过它我们可以非常方便地进行回归分析。</p>
<pre class="lang-python" data-nodeid="63091"><code data-language="python"><span class="hljs-keyword">from</span>&nbsp;sklearn.linear_model&nbsp;<span class="hljs-keyword">import</span>&nbsp;LinearRegression
<span class="hljs-comment">#&nbsp;创建线性回归实例</span>
model&nbsp;=&nbsp;LinearRegression()
</code></pre>
<p data-nodeid="63092">（5）调整 x 数组的结构，满足 LinearRegression 类的 fit 函数的要求。</p>
<pre class="lang-python" data-nodeid="63093"><code data-language="python"><span class="hljs-comment">#&nbsp;LinearRegression&nbsp;类的第一个参数接受一个&nbsp;NxM&nbsp;的二维数组，N&nbsp;代表观测记录数，M代表自变量的个数</span>
<span class="hljs-comment">#&nbsp;我们目前有&nbsp;10&nbsp;条观测记录，每条记录只有一个自变量，所以直接将其形状调整为&nbsp;10x1即可</span>
x&nbsp;=&nbsp;x.reshape([<span class="hljs-number">10</span>,&nbsp;<span class="hljs-number">1</span>])
x
</code></pre>
<p data-nodeid="63094">输出如下：</p>
<pre class="lang-java" data-nodeid="63095"><code data-language="java">array([[ <span class="hljs-number">1</span>.],
       [ <span class="hljs-number">2</span>.],
       [ <span class="hljs-number">3</span>.],
       [ <span class="hljs-number">4</span>.],
       [ <span class="hljs-number">5</span>.],
       [ <span class="hljs-number">6</span>.],
       [ <span class="hljs-number">7</span>.],
       [ <span class="hljs-number">8</span>.],
       [ <span class="hljs-number">9</span>.],
       [<span class="hljs-number">10</span>.]])
</code></pre>
<p data-nodeid="92327">可以看到，x 从一个 10 个元素的一维数组已经被成功转化为 10x1 的二维数组。</p>
<p data-nodeid="92328">（6）实现回归分析，并输出找到的最佳的 k 和 b。</p>

<pre class="lang-python" data-nodeid="63097"><code data-language="python"><span class="hljs-comment">#&nbsp;调用&nbsp;model&nbsp;对象的&nbsp;fit&nbsp;方法，即可完成回归分析</span>
model.fit(x&nbsp;,y)
<span class="hljs-comment">#&nbsp;fit&nbsp;之后，model&nbsp;对象的&nbsp;coef_&nbsp;属性是一个数组，存储的即是所有自变量的最佳系数</span>
<span class="hljs-comment">#&nbsp;这里我们只有一个自变量，直接取第一个元素即可</span>
print(<span class="hljs-string">"最佳的k:"</span>,model.coef_[<span class="hljs-number">0</span>])
<span class="hljs-comment">#&nbsp;intercept_&nbsp;属性则是偏移系数，也就是我们要找的&nbsp;b</span>
print(<span class="hljs-string">"最佳的b:"</span>,&nbsp;model.intercept_)
</code></pre>
<p data-nodeid="63098">输出如下：</p>
<pre class="lang-java" data-nodeid="63099"><code data-language="java">最佳的k: -<span class="hljs-number">0.6667878787878787</span>
最佳的b: <span class="hljs-number">11.145333333333333</span>
</code></pre>
<p data-nodeid="63100">（7）查看拟合效果。调用 fit 函数进行回归分析，虽然我们总能找到一个针对当前观测记录最优的  k 和 b，但依然可能不是完美的。毕竟观测记录一般都会有一定的误差，从我们之前画的散点图来看，观测记录只能说大概线性，而不是严格线性的（毕竟无法用一条直线把所有点连接起来）。所以每次 fit 完之后，我们都可以调用 score 函数，来看一下分析的效果如何。</p>
<pre class="lang-python" data-nodeid="63101"><code data-language="python">fit_score&nbsp;=&nbsp;model.score(x,&nbsp;y)
print(<span class="hljs-string">"fit&nbsp;的效果："</span>,fit_score)
</code></pre>
<p data-nodeid="63102">输出如下：</p>
<pre class="lang-java" data-nodeid="63103"><code data-language="java">fit 的效果： <span class="hljs-number">0.9314051422328029</span>
</code></pre>
<p data-nodeid="93159">score 返回的是 0 到 1 之间的值，所以 0.93 已经算非常不错的效果，这说明观测记录集本身也是基本线性的。</p>
<p data-nodeid="93160">（8）查看拟合出的直线。在 fit 之后，model 对象内部就记录了最佳的 k 和 b 的信息，之后我们可以调用 predict 函数，传入自变量，就可以获得对应预测出的因变量预测值。所以我们要画出拟合的直线，只需要将我们原始的 x 传入 model，拿到预测的 y ，之后和 x 结对画线即可。</p>

<pre class="lang-python" data-nodeid="63105"><code data-language="python"><span class="hljs-comment">#&nbsp;获得预测的&nbsp;y&nbsp;值</span>
y_pred&nbsp;=&nbsp;model.predict(x)
print(<span class="hljs-string">"观测记录的y："</span>,&nbsp;y)
print(<span class="hljs-string">"线性模型预测的y："</span>,&nbsp;y_pred)
<span class="hljs-comment">#&nbsp;用（x,y）&nbsp;画点击，用（x,&nbsp;y_pred画直线）&nbsp;</span>
draw_point_line(x,&nbsp;y,&nbsp;y_pred)
</code></pre>
<p data-nodeid="93991" class="te-preview-highlight">输出如下：<br>
<img src="https://s0.lgstatic.com/i/image6/M00/44/29/CioPOWC91KGAZ9sWAACNJxjxZHY689.png" alt="Drawing 27.png" data-nodeid="93996"></p>

<p data-nodeid="63107">可以看到，我们的预测值和原先的观测值仍然是有一定的误差，但也非常接近了。从图来看，我们拟合出的直线也尽可能地贴合了所有的点。</p>
<p data-nodeid="63108">（9）预测距离市中心 15 公里的房价。看到这里，相信这个任务已经非常简单了，我们只需要把 15 以一个 1x1 的二维数字传入 model 对象的 predict 即可。</p>
<pre class="lang-python" data-nodeid="63109"><code data-language="python">result&nbsp;=&nbsp;model.predict([[<span class="hljs-number">15</span>]])
<span class="hljs-comment">#&nbsp;predict的返回值是一个列表，由于我们只预测了一条，所以我们取第一条结果即可</span>
print(<span class="hljs-string">"距离市中心十五公里的房价是："</span>,&nbsp;result[<span class="hljs-number">0</span>])
</code></pre>
<p data-nodeid="63110">输出如下：</p>
<pre class="lang-java" data-nodeid="63111"><code data-language="java">距离市中心十五公里的房价是： <span class="hljs-number">1.1435151515151531</span>
</code></pre>
<h3 data-nodeid="63112">小结</h3>
<p data-nodeid="63113">至此，我们基本完成了线性回归分析的学习。</p>
<p data-nodeid="63114">回顾一下今天学习的内容，主要包含如下关键点。</p>
<ul data-nodeid="63115">
<li data-nodeid="63116">
<p data-nodeid="63117">回归分析的本质：分析变量之间的关系，通过大量的观测记录，来拟合出自变量到因变量的函数关系 f。</p>
</li>
<li data-nodeid="63118">
<p data-nodeid="63119">线性回归：最简单也是最常用的回归，代表自变量和因变量之间是线性关系，以三元回归为例，形式可以表示为 y = ax1 + bx2 +cx3 + d，要求函数关系本质就是求四个参数 a、b、c、d 的值。</p>
</li>
<li data-nodeid="63120">
<p data-nodeid="63121">用 Python 实现线性回归：</p>
<ul data-nodeid="63122">
<li data-nodeid="63123">
<p data-nodeid="63124">需要安装 scikit-learn 工具包，与 NumPy 配合完成；</p>
</li>
<li data-nodeid="63125">
<p data-nodeid="63126">自变量和因变量的观测记录需要分别存储；</p>
</li>
<li data-nodeid="63127">
<p data-nodeid="63128">创建 LinearRegression 类来进行回归分析的计算。通过 fit 函数可以完成线性函数系数的计算。在使用 fit 函数时，传入的自变量需要是 NxM 的二维数组；</p>
</li>
<li data-nodeid="63129">
<p data-nodeid="63130">fit 之后，可以调用 score 获得拟合效果的打分；</p>
</li>
<li data-nodeid="63131">
<p data-nodeid="63132">fit 之后，调用 predict 可以预测对输入的自变量的值预测出因变量的预测值。</p>
</li>
</ul>
</li>
</ul>
<p data-nodeid="63133">课后习题</p>
<p data-nodeid="63134">计算我们第三节拟合的线性模型的残差平方和，以及 k=-1,b = 20 的残差平方和。</p>
<hr data-nodeid="63135">
<p data-nodeid="63136">答案：</p>
<pre class="lang-python" data-nodeid="63137"><code data-language="python"><span class="hljs-comment">#&nbsp;本次模型的预测&nbsp;y&nbsp;值已经有了，直接相减</span>
error1&nbsp;=&nbsp;np.sum(np.square(y_pred&nbsp;-&nbsp;y))
<span class="hljs-comment">#&nbsp;计算&nbsp;k&nbsp;=&nbsp;-1,&nbsp;b=&nbsp;20&nbsp;的y的预测值</span>
y_pred2&nbsp;=&nbsp;<span class="hljs-number">-1</span>&nbsp;*&nbsp;x&nbsp;+&nbsp;<span class="hljs-number">20</span>
error2&nbsp;=&nbsp;np.sum(np.square(y_pred2&nbsp;-&nbsp;y))
print(<span class="hljs-string">"模型拟合的残差平方和："</span>,&nbsp;error1)
print(<span class="hljs-string">"k=-1,b=20&nbsp;的残差平方和："</span>,error2)
</code></pre>
<p data-nodeid="63138">输出如下：</p>
<pre class="lang-java" data-nodeid="63139"><code data-language="java">模型拟合的残差平方和： <span class="hljs-number">2.7013587878787853</span>
k=-<span class="hljs-number">1</span>,b=<span class="hljs-number">20</span> 的残差平方和： <span class="hljs-number">6149.662</span>
</code></pre>

---

### 精选评论


