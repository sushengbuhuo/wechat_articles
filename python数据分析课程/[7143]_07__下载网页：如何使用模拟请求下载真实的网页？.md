<p data-nodeid="24538" class="">在上一讲中，我初步介绍了爬虫的原理以及要实现爬虫的三个主要步骤：下载网页-分析网页-保存数据。</p>
<p data-nodeid="24539">今天我们就从第一步开始实操，如何使用 Python 下载网页。</p>
<h3 data-nodeid="24540">网页是什么</h3>
<p data-nodeid="24541">上一讲我有介绍浏览器画网页的流程，是浏览器将用户输入的网址告诉网站的服务器，然后网站的服务器将网址对应的网页返回给浏览器，由浏览器将网页画出来。</p>
<p data-nodeid="24542"><strong data-nodeid="24664">这里所说的网页，一般都是一个后缀名为 html 的文件</strong>。</p>
<p data-nodeid="24543">网页文件和我们平时打交道的文件没什么不同，平时我们知道 Word 文件，后缀名为 .doc， 通过 Word 可以打开。图片文件后缀名为 .jpg，通过 Photoshop 可以打开；而网页则是后缀名为 .html，通过浏览器可以打开的文件。</p>
<p data-nodeid="24544">网页文件本质也是一种文本文件，为了能够让文字和图片呈现各种各样不同的样式，网页文件通过一种叫作 HTML 语法的标记规则对原始文本进行了标记。</p>
<h4 data-nodeid="24545">手动下载网页</h4>
<p data-nodeid="24546">我们以煎蛋网为例体会一下网页的实质，使用浏览器打开这个链接<a href="http://jandan.net/?fileGuid=xxQTRXtVcqtHK6j8" data-nodeid="24671">http://jandan.net/</a>可以看到如下界面。可以看到，第一条新闻的标题前缀是：大吐槽。网页内容可能会随时间变化，这里你只需要注意第一条新闻的前几个字（暗号）即可，下同。</p>
<p data-nodeid="24547"><img src="https://s0.lgstatic.com/i/image6/M01/3A/83/Cgp9HWB_4gaAPkpdABH-XPiwU5k779.png" alt="Drawing 0.png" data-nodeid="24675"></p>
<p data-nodeid="24548">在空白区域点击右键，另存为，并在保存类型中选择：仅 HTML。</p>
<p data-nodeid="24549"><img src="https://s0.lgstatic.com/i/image6/M01/3A/83/Cgp9HWB_4g2AGtaNAAHJ5DbA2QY668.png" alt="Drawing 1.png" data-nodeid="24679"></p>
<p data-nodeid="24550">接下来回到桌面，可以看到网页已经被保存到桌面了，后缀名是 html，这个就是我们所说的网页文件。</p>
<h4 data-nodeid="24551">网页内容初探</h4>
<p data-nodeid="24552">我们右键刚下载的文件，选择用 VS Code 打开，打开后的文件内容如下图所示。</p>
<p data-nodeid="24553"><img src="https://s0.lgstatic.com/i/image6/M01/3A/83/Cgp9HWB_4kaAT849AAgd2HybV0k643.png" alt="Drawing 2.png" data-nodeid="24685"></p>
<p data-nodeid="24554">这就是网页文件的实际内容（未被浏览器画出来之前）。现在先不用管看不懂的代码，还记得我们看到的第一条新闻吗？“大吐槽………………”。（你的暗号）</p>
<p data-nodeid="24555">我们在 VS Code 中通过 CTRL + F 调出搜索面板，搜索“大吐槽”（暗号）。可以看到成功找到了这条新闻，虽然被很多不认识的代码包围，但这也可以确定，<strong data-nodeid="24692">我们看到的煎蛋网的主页确实就是这个 html 文件</strong>。</p>
<p data-nodeid="24556"><img src="https://s0.lgstatic.com/i/image6/M01/3A/8C/CioPOWB_4luAAi4SAAcBMOTTAsY956.png" alt="Drawing 3.png" data-nodeid="24695"></p>
<p data-nodeid="24557">html 文件里有文字，也有很多我们看不懂的标记和代码，浏览器正是通过这些标记来将文本画成丰富多彩的网页。</p>
<p data-nodeid="24558">现在，我们已经确认了 html 文件中包含我们所需要的信息，接下来我们将会学习如何使用Python 代码来下载 html 文件。</p>
<h3 data-nodeid="24559">如何实现下载普通网页</h3>
<p data-nodeid="24560">Python 以系统类的形式提供了下载网页的功能，放在 urllib3 这个模块中。这里面有比较多的类，我们并不需要逐一都学会，只需要记住主要的用法即可。</p>
<h4 data-nodeid="24561">（1）获取网页内容</h4>
<p data-nodeid="24562">还是以煎蛋网为例，首页的网址是：<a href="http://jandan.net/?fileGuid=xxQTRXtVcqtHK6j8" data-nodeid="24704">http://jandan.net/</a>。在我写作这一讲的时候，排在第一的新闻是：“引发普通感冒的鼻病毒会将新冠病毒排挤出细胞！”。</p>
<p data-nodeid="24563">煎蛋又更新了新的新闻（如下图），你记住你当时的第一条新闻题目即可。我们待会儿会在我们下载的网页中搜索这个标题来验证我们下载的正确性。</p>
<p data-nodeid="24564"><img src="https://s0.lgstatic.com/i/image6/M01/3A/83/Cgp9HWB_4mSAIiZpABS7tDgmvH8873.png" alt="Drawing 4.png" data-nodeid="24709"></p>
<p data-nodeid="24565">在学习课程的工作目录下新建一个文件夹 chapter07（我的如下图），如果之前没有固定存储途径可以在桌面新建文件夹 chapter07，用于存放本讲的相关代码与文件。</p>
<p data-nodeid="24566"><img src="https://s0.lgstatic.com/i/image6/M01/3A/83/Cgp9HWB_4mqAflX7AADdJAvvA4s470.png" alt="Drawing 5.png" data-nodeid="24713"></p>
<p data-nodeid="24567">之后打开 VS Code。选择文件-&gt;打开文件夹，选择刚才新建的 chapter07 文件夹（今天之所以要建文件夹是方便查看下载的文件）。</p>
<p data-nodeid="24568">打开文件夹后，按CTRL + P，调出命令菜单，输入&gt;Jup， 在命令菜单中选择：Create New Blank Notebook，如下图所示。</p>
<p data-nodeid="24569"><img src="https://s0.lgstatic.com/i/image6/M01/3A/83/Cgp9HWB_4nGAYuTyAADuaT_Fkdw061.png" alt="Drawing 6.png" data-nodeid="24718"></p>
<p data-nodeid="24570">之前按 CTRL + S 保存，在弹出的对话框中选择保存位置为刚才创建的文件夹，文件名保存为 chapter07.ipynb， 如下图所示。</p>
<p data-nodeid="24571"><img src="https://s0.lgstatic.com/i/image6/M01/3A/8C/CioPOWB_4neAWm2oAAE_xO2F-Kk679.png" alt="Drawing 7.png" data-nodeid="24722"></p>
<p data-nodeid="24572">保存完毕后如图所示：</p>
<p data-nodeid="24573"><img src="https://s0.lgstatic.com/i/image6/M01/3A/8C/CioPOWB_4nyAHuRHAACv0NadkzA658.png" alt="Drawing 8.png" data-nodeid="24726"></p>
<p data-nodeid="24574">在默认创建的 Cell 中，输入如下代码：</p>
<pre class="lang-python" data-nodeid="24575"><code data-language="python"><span class="hljs-comment"># 导入 urllib3 模块的所有类与对象</span>
<span class="hljs-keyword">import</span> urllib3

<span class="hljs-comment"># 将要下载的网址保存在 url 变量中，英文一般用 url 表示网址的意思</span>
url = <span class="hljs-string">"http://jandan.net/p/date/2021/03/23"</span>

<span class="hljs-comment"># 创建一个 PoolManager 对象，命名为 http</span>
http = urllib3.PoolManager()

<span class="hljs-comment"># 调用 http 对象的 request 方法，第一个参数传一个字符串 "GET"</span>
<span class="hljs-comment"># 第二个参数则是要下载的网址，也就是我们的 url 变量</span>
<span class="hljs-comment"># request 方法会返回一个 HTTPResponse 类的对象，我们命名为 response</span>
response = http.request(<span class="hljs-string">"GET"</span>, url)

<span class="hljs-comment"># 获取 response 对象的 data 属性，存储在变量 response_data 中</span>
response_data = response.data

<span class="hljs-comment"># 调用 response_data 对象的 decode 方法，获得网页的内容，存储在 html_content </span>
<span class="hljs-comment"># 变量中</span>
html_content = response_data.decode()

<span class="hljs-comment"># 打印 html_content</span>
print(html_content)
</code></pre>
<p data-nodeid="24576">上述代码就完成了一个完成的网页下载的功能。其中有几个额外要注意的点：</p>
<ul data-nodeid="24577">
<li data-nodeid="24578">
<p data-nodeid="24579">我们创建 PoolManager的时候，写的是 urllib3.PoolManager，这里是因为我们导入了 urllib3 的所有类与函数。所以在调用这个模块的所有函数和类的前面都需要加模块名，并用点符号连接。</p>
</li>
<li data-nodeid="24580">
<p data-nodeid="24581">response 对象的 data 属性也是一个对象，是一个 bytes 类型的对象。通过调用 decode 方法，可以转化成我们熟悉的字符串。</p>
</li>
</ul>
<p data-nodeid="24582">执行上述代码，可以看到打印出了非常多的内容，而且很像我们第一部分手动保存的网页，这说明目前 html_content 变量中保存的就是我们要下载的网页内容。</p>
<p data-nodeid="24583"><img src="https://s0.lgstatic.com/i/image6/M01/3A/83/Cgp9HWB_4oeAUJWUAARROv7hjcE739.png" alt="Drawing 9.png" data-nodeid="24736"></p>
<h4 data-nodeid="24584">（2）将网页保存到文件</h4>
<p data-nodeid="24585">现在 html_content 已经是我们想要的网页内容，对于完成下载只差最后一步，就是将其保存成文件。其实这一步已经和保存网页无关的，而是<strong data-nodeid="24745">我们如何把一个字符串保存成一个文件</strong>。</p>
<p data-nodeid="24586">Python 中，读取文件和保存文件都是通过文件对象来完成的。接下来，我们通过实际的例子来学习这个技术。</p>
<p data-nodeid="24587">新建 Cell，输入以下代码：</p>
<pre class="lang-python" data-nodeid="24588"><code data-language="python"><span class="hljs-comment"># 调用 open 函数，三个参数都是字符串类型，第一个参数为要操作的文件名</span>
<span class="hljs-comment"># 第二个参数代表模式，w 表示写入文件，r 表示读取文件</span>
<span class="hljs-comment"># 第三个参数表示编码格式，一般有中文的认准 utf-8 就好</span>
<span class="hljs-comment"># open 函数返回一个文件对象，我们存储在 fo 变量中</span>
fo&nbsp;=&nbsp;open(<span class="hljs-string">"jiandan.html"</span>,<span class="hljs-string">"w"</span>,&nbsp;encoding=<span class="hljs-string">"utf-8"</span>)
<span class="hljs-comment"># 调用文件对象的 write 方法，将我们上面存储着网页内容的字符春变量，html_content </span>
<span class="hljs-comment"># 作为参数</span>
fo.write(html_content)
<span class="hljs-comment"># 关闭文件对象</span>
fo.close()
</code></pre>
<p data-nodeid="24589">执行完上述代码后，可以在 VS Code 的左侧边栏中看到，chapter07 文件夹下多了一个 jiandan.html 的文件，这个就是我们用刚才 Python 代码保存的文件。</p>
<p data-nodeid="24590">打开就可以看到熟悉的网页内容了，如下图所示。</p>
<p data-nodeid="24591"><img src="https://s0.lgstatic.com/i/image6/M01/3A/83/Cgp9HWB_4pKAMD6qAAnuV1QlY48401.png" alt="Drawing 10.png" data-nodeid="24752"></p>
<p data-nodeid="24592">接下来，我们验证一下我们下载的这个网页是不是我们通过浏览器看到的。还记得第一条新闻的标题吗？——“引发普通感冒的鼻病毒会将新冠病毒排挤出细胞”（暗号）。</p>
<p data-nodeid="24593">我们在 jiandan.html 文件中搜索这句话，可以成功搜索到，这说明这就是我们看到的网页内容。</p>
<p data-nodeid="24594"><img src="https://s0.lgstatic.com/i/image6/M01/3A/83/Cgp9HWB_4pqAAoofAAn2Iz-2hdU442.png" alt="Drawing 11.png" data-nodeid="24757"></p>
<p data-nodeid="24595">至此，我们正式完成了通过代码下载网页的功能，回过头去看，其实并没多少行代码，是不是逐渐开始感受到 Python 的强大了呢。</p>
<h4 data-nodeid="24596">（3）让我们的代码更加通用</h4>
<p data-nodeid="24597">刚才我们在两个 cell 中分别实现了将网页保存成一个字符串，以及将字符串保存为一个文件。如果我们要抓取新的网页，要么直接修改之前的代码，要么就需要拷贝一份代码出来。</p>
<p data-nodeid="24598">这两种方式都不是很好，基于我们之前学习的内容，对于有一定通用度的代码我们可以将其改写为函数，来方便后续使用。</p>
<p data-nodeid="24599">改写之后的代码如下：</p>
<pre class="lang-python" data-nodeid="24600"><code data-language="python"><span class="hljs-comment"># 第一个函数，用来下载网页，返回网页内容</span>
<span class="hljs-comment"># 参数 url 代表所要下载的网页网址。</span>
<span class="hljs-comment"># 整体代码和之前类似</span>
<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">download_content</span>(<span class="hljs-params">url</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;http&nbsp;=&nbsp;urllib3.PoolManager()
&nbsp;&nbsp;&nbsp;&nbsp;response&nbsp;=&nbsp;http.request(<span class="hljs-string">"GET"</span>,&nbsp;url)
&nbsp;&nbsp;&nbsp;&nbsp;response_data&nbsp;=&nbsp;response.data
&nbsp;&nbsp;&nbsp;&nbsp;html_content&nbsp;=&nbsp;response_data.decode()
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span>&nbsp;html_content
<span class="hljs-comment"># 第二个函数，将字符串内容保存到文件中</span>
<span class="hljs-comment"># 第一个参数为所要保存的文件名，第二个参数为要保存的字符串内容的变量</span>
<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">save_to_file</span>(<span class="hljs-params">filename,&nbsp;content</span>):</span>
&nbsp;&nbsp;&nbsp;&nbsp;fo&nbsp;=&nbsp;open(filename,<span class="hljs-string">"w"</span>,&nbsp;encoding=<span class="hljs-string">"utf-8"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;fo.write(content)
&nbsp;&nbsp;&nbsp;&nbsp;fo.close()
url&nbsp;=&nbsp;<span class="hljs-string">"http://jandan.net/"</span>
<span class="hljs-comment"># 调用 download_content 函数，传入 url，并将返回值存储在html_content </span>
<span class="hljs-comment"># 变量中</span>
html_content&nbsp;=&nbsp;download_content(url)
<span class="hljs-comment"># 调用 save_to_file 函数，文件名指定为 jiandan.html, 然后将上一步获得的</span>
<span class="hljs-comment"># html_content 变量作为第二个参数传入</span>
save_to_file(<span class="hljs-string">"jiandan.html"</span>,&nbsp;html_content)
</code></pre>
<p data-nodeid="24601">这样改写之后，我们在抓取新的网页的时候就可以使用 download_content 函数和 save_to_file 函数快速完成了，不再需要去写里面复杂的实现。</p>
<h3 data-nodeid="24602">如何实现下载动态网页</h3>
<p data-nodeid="24603">urllib3 很强大，但是却不能一劳永逸地解决网页下载问题。对于煎蛋这类普通网页，urllib3 可以表现更好，但是有一种类型的网页，它的数据是动态加载的，就是先出现网页，然后延迟加载的数据，那 urllib3 可能就有点力不从心了。</p>
<p data-nodeid="24604">我们以豆瓣的电视剧网页为例，网址为：<a href="https://movie.douban.com/tv?fileGuid=xxQTRXtVcqtHK6j8" data-nodeid="24775">https://movie.douban.com/tv</a></p>
<p data-nodeid="24605"><img src="https://s0.lgstatic.com/i/image6/M01/3A/84/Cgp9HWB_4qiAaUEIACqVoWGYHNY522.png" alt="Drawing 12.png" data-nodeid="24778"></p>
<p data-nodeid="24606">现在，我们来使用刚才定义的两个函数来下载一下这个网页。</p>
<pre class="lang-python" data-nodeid="24607"><code data-language="python">url&nbsp;=&nbsp;<span class="hljs-string">"https://movie.douban.com/tv"</span>
html_content&nbsp;=&nbsp;download_content(url)
save_to_file(<span class="hljs-string">"douban_tv.html"</span>,&nbsp;html_content)
</code></pre>
<p data-nodeid="24608">代码很简单，就是把豆瓣电视剧的网页下载到 douban_tv.html 这个文件中。执行代码，可以在 VS Code 左边的文件夹视图中看到成功生成了douban_tv.html 这个文件，这说明网页已经下载成功。如下图所示：</p>
<p data-nodeid="24609"><img src="https://s0.lgstatic.com/i/image6/M01/3A/8C/CioPOWB_4q-AOhpEAAFGj9J4_XA215.png" alt="Drawing 13.png" data-nodeid="24787"></p>
<p data-nodeid="24610">现在我们在 VS Code 中打开这个网页，搜索上图中出现的电视剧：“山河令”。这次却神奇的搜不到了，事实上，你会发现我们在网页看到的电视剧名字都搜不到。</p>
<p data-nodeid="24611"><img src="https://s0.lgstatic.com/i/image6/M00/3A/8C/CioPOWB_4r2Aa-vCAAMY-Ndhz6k538.png" alt="Drawing 14.png" data-nodeid="24791"></p>
<p data-nodeid="24612">为什么我们明明下载到了网页但是却搜不到电视剧呢？造成这个现象的原因是豆瓣电视剧网页中的电视剧列表的部分是动态加载的，所以我们用 urllib3 去直接下载，只能下载到一个壳网页，没有里面的列表内容。这种网页内部的数据是动态加载的网页，我们统一称之为动态网页。</p>
<p data-nodeid="24613">动态网页应该怎么抓取呢？回过头去想，一个网页不管再怎么动态，最终都是要展示给用户看的，所以浏览器应该是最知道网页内容是什么的角色。如果我们可以使用代码控制浏览器来帮我们下载网页，应该就可以解决动态网页的抓取问题。</p>
<p data-nodeid="24614">接下来我们就介绍使用 Python 来控制浏览器的利器：selenium。</p>
<h4 data-nodeid="24615">安装 selenium</h4>
<p data-nodeid="24616">selenium 不属于 Python 的系统库，所以要使用这个库需要先进行安装。</p>
<p data-nodeid="24617">我们安装 Python 的库一般通过 Anaconda 的命令行。既然是模拟浏览器，我们的电脑首先要先有浏览器。这里我们以 Chrome 为例。所以在一切开始之前，你需要确保你电脑上安装了 Chrome。</p>
<p data-nodeid="24618">在课前准备环节，我们已经安装了 Anaconda 套件，现在我们去开始菜单（或者在桌面状态下按 Win 键）找到 Anaconda 3 文件夹，并点击文件夹中的 Anaconda Prompt 程序。Mac 用终端即可。</p>
<p data-nodeid="24619"><img src="https://s0.lgstatic.com/i/image6/M01/3A/84/Cgp9HWB_4sqAWoNWAAi_FDh9ttA555.png" alt="Drawing 15.png" data-nodeid="24801"></p>
<p data-nodeid="24620">启动之后会出现一个黑乎乎的界面，这个就是 Anaconda 的命令行。</p>
<p data-nodeid="24621"><img src="https://s0.lgstatic.com/i/image6/M00/3A/8C/CioPOWB_4tGAe6uPAACGepSwh_M874.png" alt="Drawing 16.png" data-nodeid="24805"></p>
<p data-nodeid="24622">在这个命令行，我们可以输入 conda install xxx 来安装 Python 的扩展库。</p>
<p data-nodeid="24623">比如在这个例子中，我们输入 conda install selenium，回车。界面会变得如下所示，询问我们是否要确认安装，输入 y 继续回车就可以。</p>
<p data-nodeid="24624"><img src="https://s0.lgstatic.com/i/image6/M01/3A/84/Cgp9HWB_4tiAI_8uAAEOw_EkcCc855.png" alt="Drawing 17.png" data-nodeid="24810"></p>
<p data-nodeid="24625">安装完毕后命令行窗口会回到待输入命令的状态，此时就可以关闭了。</p>
<p data-nodeid="24626">回到 chaper07.ipynb，新建 Cell，输入以下的测试代码：</p>
<pre class="lang-python" data-nodeid="24627"><code data-language="python"><span class="hljs-comment"># 从 selenium 库中导入 webdriver 类</span>
<span class="hljs-keyword">from</span>&nbsp;selenium&nbsp;<span class="hljs-keyword">import</span>&nbsp;webdriver
<span class="hljs-comment"># 创建一个 Chrome 浏览器的对象</span>
brow&nbsp;=&nbsp;webdriver.Chrome()
<span class="hljs-comment"># 使用 Chrome 对象打开 url(就是刚才豆瓣电视剧的 url)</span>
brow.get(url)
</code></pre>
<p data-nodeid="24628">运行代码，会自动打开一个 Chrome 浏览器的窗口，并展示 url 对应的网页。同时还会有一个提示，说明这个浏览器窗口是在被程序控制的，如下图所示。</p>
<p data-nodeid="24629"><img src="https://s0.lgstatic.com/i/image6/M01/3A/84/Cgp9HWB_4uCAWHwoACcJY0tnQ7Q824.png" alt="Drawing 18.png" data-nodeid="24816"></p>
<p data-nodeid="24630">如果代码运行出错，提示找不到 chromedriver。那说明你安装的 selenium 版本缺少 chromedriver, 可以按如下方式操作：</p>
<ol data-nodeid="24631">
<li data-nodeid="24632">
<p data-nodeid="24633">重新按照刚才的方法打开 Anaconda Prompt，输入 pip install --upgrade --force-reinstall chromedriver-binary-auto 回车执行安装。</p>
</li>
<li data-nodeid="24634">
<p data-nodeid="24635">在上面的代码增加一行 import chromedriver_binary 添加完毕后如下所示。</p>
</li>
</ol>
<pre class="lang-python" data-nodeid="24636"><code data-language="python"><span class="hljs-comment"># 从 selenium 库中导入 webdriver 类</span>
<span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver
<span class="hljs-comment"># 导入 chromedriver</span>
<span class="hljs-keyword">import</span>&nbsp;chromedriver_binary
<span class="hljs-comment"># 创建一个 Chrome 浏览器的对象</span>
brow = webdriver.Chrome()
<span class="hljs-comment"># 使用 Chrome 对象打开 url(就是刚才豆瓣电视剧的 url)</span>
brow.get(url)
</code></pre>
<h4 data-nodeid="24637">使用 selenium 下载动态网页</h4>
<p data-nodeid="24638">如果刚才的代码已经运行成功并打开了 Chrome 的界面的话，那我们离最后的下载动态网页已经不远了。在我们通过 Chrome 对象打开了一个网页之后，只需要访问 Chrome 对象的 page_source 属性即可拿到网页的内容。</p>
<p data-nodeid="24639">代码如下所示：</p>
<pre class="lang-python" data-nodeid="24640"><code data-language="python"><span class="hljs-comment"># 从 selenium 库中导入 webdriver 类</span>
<span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver
<span class="hljs-comment"># 创建一个 Chrome 浏览器的对象</span>
brow = webdriver.Chrome()
<span class="hljs-comment"># 使用 Chrome 对象打开 url(就是刚才豆瓣电视剧的 url)</span>
brow.get(url)
<span class="hljs-comment"># 访问 Chrome 对象的 page_source 属性，并存储在 html_content 变量中</span>
html_content&nbsp;=&nbsp;brow.page_source
<span class="hljs-comment"># 调用我们之前定义的 save_to_file 函数，这次我们保存为 double_tv1.html</span>
<span class="hljs-comment"># 要保存的内容就是 html_content</span>
save_to_file(<span class="hljs-string">"douban_tv1.html"</span>,&nbsp;html_content)
</code></pre>
<p data-nodeid="24641">运行代码，可以看到 Chrome 被打开并加载网页，等电视剧列表都加载完之后，在 VS Code 中可以看到 double_tv1.html 也被成功创建。</p>
<p data-nodeid="24642"><img src="https://s0.lgstatic.com/i/image6/M01/3A/8C/CioPOWB_4uqAcpT-AAMbpCdTW44911.png" alt="Drawing 19.png" data-nodeid="24832"></p>
<p data-nodeid="24643">这个时候我们去这个文件搜索山河令，发现已经有结果了，在这个 html 文件中已经有了所有电视剧的信息。</p>
<p data-nodeid="24644"><img src="https://s0.lgstatic.com/i/image6/M00/3A/84/Cgp9HWB_4vSAXn6MAATchRuOYm8086.png" alt="Drawing 20.png" data-nodeid="24836"></p>
<p data-nodeid="24645">至此，我们也实现了对于动态内容网页的下载功能。</p>
<h3 data-nodeid="24646">小结</h3>
<p data-nodeid="24647">这一讲里，我们首先对网页有了个比较初步的介绍，说明了网页本质上是一种被 html 语法规则标记的文本文件，并通过在浏览器的空白区域点击右键-另存为，我们可以将网页保存下来。</p>
<p data-nodeid="24648">之后，我们通过 urllib3 来下载普通网页，并将整个过程写成了两个比较通用的函数：download_content 和 save_to_file。</p>
<p data-nodeid="24649">而对于部分包含动态内容的网页，比如豆瓣电视剧的主页、电视剧列表是动态请求的。我们通过 selenium 模块操作浏览器的形式实现了动态网页的下载。</p>
<p data-nodeid="24650">课后练习：</p>
<p data-nodeid="24651">煎蛋的内容是分页存储的，规则是：第一页是 jiandan.net ，第二页是 jiandan.net/page/2，第三页是 jiandan.net/page/3 ....</p>
<p data-nodeid="24652">请你编写代码，下载煎蛋最近五页的内容。</p>
<hr data-nodeid="24653">
<p data-nodeid="24654">答案：</p>
<pre class="lang-python te-preview-highlight" data-nodeid="24655"><code data-language="python"><span class="hljs-comment"># i 从 1 到 5循环，</span>
<span class="hljs-keyword">for</span>&nbsp;i&nbsp;<span class="hljs-keyword">in</span>&nbsp;range(<span class="hljs-number">1</span>,&nbsp;<span class="hljs-number">6</span>):
&nbsp;&nbsp;&nbsp;&nbsp;url&nbsp;=&nbsp;<span class="hljs-string">"http://jiandan.net"</span>
    <span class="hljs-comment"># 第一页不需要加/page/x 后缀，所以判断大于 1 才加</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span>&nbsp;i&nbsp;&gt;&nbsp;<span class="hljs-number">1</span>:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;url&nbsp;=&nbsp;url&nbsp;+&nbsp;<span class="hljs-string">"/page/"</span>&nbsp;+&nbsp;str(i)
&nbsp;&nbsp;&nbsp;&nbsp;html_content&nbsp;=&nbsp;download_content(url)
&nbsp;&nbsp;&nbsp;&nbsp;save_to_file(<span class="hljs-string">"jiandan_"</span>+str(i)&nbsp;+&nbsp;<span class="hljs-string">".html"</span>,&nbsp;html_content)
</code></pre>

---

### 精选评论

##### **威：
> 老师，我使用了selenium下载豆瓣网页后，还是查找不到对应的电视剧名称，程序运行也没报错，chromedriver也按照说明安装了

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 加延迟 time.sleep(2) 延迟去拿内容

##### *路：
> 如果能快点更新就好了，期待老师的表演😂😂

