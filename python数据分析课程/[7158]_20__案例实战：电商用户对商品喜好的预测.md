<p data-nodeid="3">在第 16 讲中，我们使用 pandas 对电商的用户行为数据做了一些简单的分析，并为短信营销提供了人群维度和时间维度的参考。不知你在分析完后是否有意犹未尽的感觉，是否总觉得那份数据集中还隐藏了很多的信息在向你招手？</p>
<p data-nodeid="4">现在我们已经完成了 NumPy 相关的学习，习得了一身从数值维度切入进行数据分析的本领。本讲我将会带你继续杀回电商数据集，尝试更进一步，挖掘出更多的信息。</p>
<h3 data-nodeid="5">任务背景</h3>
<p data-nodeid="6">通过上一次你对营销短信的人群与时段的出色分析，阿普闪购的推广活动圆满达成了目标。当日订单量不断刷新新高，但与此同时，另一个问题也暴露出来：部分供应商对此次活动准备不足，导致生产跟不上了，很多商品发货周期变长，用户的整体满意度也受到了影响。</p>
<p data-nodeid="7">接下来，即将迎来下一次的大促节日，这次节日同以往不同，用户在开始活动的前三天内只能浏览商品、收藏或者添加购物车，不能下单，等三天过后才开始秒杀下单。为了不再重蹈之前准备不足的覆辙。你作为阿普闪购的数据分析师再一次临危受命。</p>
<p data-nodeid="8">这一次，任务很明确：<strong data-nodeid="199">根据用户前三天预热期的行为记录，预测出每款商品的销量，这样可以提前知会供应商按照预测销量进行准备，避免之前准备不足的问题。</strong></p>
<p data-nodeid="9">这次，我们能拿到的数据和上次也是一样的。</p>
<ul data-nodeid="10">
<li data-nodeid="11">
<p data-nodeid="12">用户行为表：最近 6 个月的用户行为记录。</p>
</li>
<li data-nodeid="13">
<p data-nodeid="14">vip 会员表：用户 vip 会员开通情况。</p>
</li>
<li data-nodeid="15">
<p data-nodeid="16">用户信息表：用户的相关信息。</p>
</li>
</ul>
<p data-nodeid="17">三个表的字段说明如下所示。</p>
<p data-nodeid="18">用户行为表：</p>
<p data-nodeid="2438" class=""><img src="https://s0.lgstatic.com/i/image6/M01/44/B4/Cgp9HWDAdPyAFRqBAAB6v0DV6Vg387.png" alt="Drawing 0.png" data-nodeid="2441"></p>

<p data-nodeid="20">vip 会员表：</p>
<p data-nodeid="3416" class=""><img src="https://s0.lgstatic.com/i/image6/M01/44/BC/CioPOWDAdQGAFd2kAAA9MYvr6j0538.png" alt="Drawing 1.png" data-nodeid="3419"></p>

<p data-nodeid="22">用户信息表：</p>
<p data-nodeid="4398" class=""><img src="https://s0.lgstatic.com/i/image6/M01/44/BC/CioPOWDAdQeAJxxmAACGe0G0_-g395.png" alt="Drawing 2.png" data-nodeid="4401"></p>

<h3 data-nodeid="24">问题分析</h3>
<p data-nodeid="25" class="">从任务说明中可以发现，我们的核心任务是从其他行为（点击、添加收藏、添加购物车）预测商品的销量。这和我们上一次的分析维度不同，这次主要是从商品维度进行分析。</p>
<p data-nodeid="26">我们有三个数据源，VIP 会员表和用户信息表基本都是用户的信息。而用户行为表中则包含了商品的信息，所以这次我们重点从用户行为表入手。但面临的挑战是：用户行为表是面向行为的，每一行代表一个用户的一次行为，可能是点击、可能是加入购物车，等等。我们首先需要从这个表中抽象出一个面向商品的表，然后再尝试建立预测销量的模型。</p>
<h3 data-nodeid="27">数据清洗</h3>
<p data-nodeid="28">接下来进入数据清洗环节。在工作目录新建文件夹 chapter20，之后将 chapter16 中用到的 data_set 文件夹拷贝到 chapter20 文件夹中。</p>
<p data-nodeid="29">在 VS code 打开chapter20 文件夹，并新建 Notebook，保存为 chapter20.ipynb。</p>
<p data-nodeid="30">首先，导入必要的工具包以及加载用户行为表。</p>
<pre class="lang-python" data-nodeid="31"><code data-language="python"><span class="hljs-keyword">import</span>&nbsp;pandas&nbsp;<span class="hljs-keyword">as</span>&nbsp;pd
<span class="hljs-keyword">import</span>&nbsp;numpy&nbsp;<span class="hljs-keyword">as</span>&nbsp;np
df_user_log&nbsp;=&nbsp;pd.read_csv(<span class="hljs-string">"data_set/user_behavior_time_resampled.csv"</span>)
df_vip_user&nbsp;=&nbsp;pd.read_csv(<span class="hljs-string">"data_set/vip_user.csv"</span>)
</code></pre>
<p data-nodeid="32">你可能已经忘记了 user_log 表的基本情况，我们打印出来看一下。</p>
<pre class="lang-python" data-nodeid="33"><code data-language="python">df_user_log
</code></pre>
<p data-nodeid="5384" class="">输出为：<br>
<img src="https://s0.lgstatic.com/i/image6/M01/44/B4/Cgp9HWDAdROAe26AAAD_78GuvhY590.png" alt="Drawing 3.png" data-nodeid="5389"></p>

<p data-nodeid="35">该表一共有 1098w 行记录，用户的每一次行为是就是一行。其中本次任务我们主要感兴趣的是 item_id（用于区分商品），以及 action_type（用户行为的标记）。</p>
<p data-nodeid="36">现在我们来查看一下这个表的缺失值记录。</p>
<pre class="lang-python" data-nodeid="37"><code data-language="python">df_user_log.isnull().sum()
</code></pre>
<p data-nodeid="38">输出：</p>
<pre class="lang-java" data-nodeid="39"><code data-language="java">user_id            <span class="hljs-number">0</span>
item_id            <span class="hljs-number">0</span>
cat_id             <span class="hljs-number">0</span>
seller_id          <span class="hljs-number">0</span>
brand_id       <span class="hljs-number">18132</span>
time_stamp         <span class="hljs-number">0</span>
action_type        <span class="hljs-number">0</span>
timestamp          <span class="hljs-number">0</span>
click              <span class="hljs-number">0</span>
cart               <span class="hljs-number">0</span>
order              <span class="hljs-number">0</span>
fav                <span class="hljs-number">0</span>
dtype: int64
</code></pre>
<p data-nodeid="40">brand_id 有 18132 个缺失值，但我们这次的任务并不需要考虑 brand_id，所以不需要处理。</p>
<p data-nodeid="41">下一步就是查看 action_type 的取值，从数据集的描述来看，action_type 只存在 0-3 的取值，由于这个字段对我们很重要，所以我们需要确认是否存在异常值。</p>
<pre class="lang-python" data-nodeid="42"><code data-language="python">df_user_log[<span class="hljs-string">"action_type"</span>].value_counts()
</code></pre>
<p data-nodeid="43">输出为：</p>
<pre class="lang-python" data-nodeid="44"><code data-language="python"><span class="hljs-number">0</span>    <span class="hljs-number">9709458</span>
<span class="hljs-number">2</span>     <span class="hljs-number">659577</span>
<span class="hljs-number">3</span>     <span class="hljs-number">600738</span>
<span class="hljs-number">1</span>      <span class="hljs-number">15293</span>
Name: action_type, dtype: int64
</code></pre>
<p data-nodeid="45">可以看到，action_type 的取值没有异常值，并且点击行为都远高于其他三种行为，其中加到购物车的数量最少。</p>
<p data-nodeid="46">目前看来数据集已经基本满足我们的要求，接下来进入下一个阶段：特征工程。</p>
<h3 data-nodeid="47">特征工程</h3>
<h4 data-nodeid="48">为什么需要特征工程</h4>
<p data-nodeid="49">在绝大多数需要建立模型的数据分析任务里，特征工程都是必不可少的环节。顾名思义，特征工程就是指准备特征、筛选特征的工作，只是这项工作具有一定的重要性和复杂度，所以也会叫作工程。</p>
<p data-nodeid="50">以线性回归模型为例，通过上一节的学习，我们都知道线性回归模型本质上是从找出自变量和因变量的关系。现实问题中，因变量是很好找的，因为往往因变量就是我们要预测的任务，比如在这个例子中，因变量就是我们的预测目标：商品的销量。</p>
<p data-nodeid="51">另一方面，<strong data-nodeid="257">自变量，也称为模型的特征</strong>，往往没那么明显。一个原因是因变量往往会受非常多的因素所影响，有直接的，也有间接的。另一个原因是我们拿到的数据集上往往没有现成的特征给我们。这需要我们从原始数据集中经过分析、变换之后才能得到我们想要的特征。</p>
<h4 data-nodeid="52">特征分析</h4>
<p data-nodeid="53">特征工程的第一步，是根据我们已有的数据（哪怕不是直接可用的数据）以及我们的因变量，来预设出一组特征。第二步，就是从原始数据中计算出这些特征。第三步则是用这些特征对模型进行训练，如何训练出的模型有问题，则需要重新回到第一步，尝试重新尝试其他的特征。</p>
<p data-nodeid="54">我们这次的因变量是商品的销量，能够作为参考的数据是用户在预热期的三天的行为。用户在浏览商品时，不能下单的话，主要的行为就是：点击商品、把商品添加到购物车，以及收藏商品。那对于商品来说，商品的点击数、加购物车的次数，以及收藏的次数是否可以作为销量的特征呢？</p>
<p data-nodeid="55">判断特征是否有效可以从特征的值改变是否会影响因变量的值这个标准出发。这三个特征，不管是点击数，还是收藏数，还是加购物车的次数越多，就说明商品越受欢迎，则说明越可能有用户来购买，反之亦然。所以这三个特征都可以作为我们这次任务的特征。</p>
<p data-nodeid="56">除了商品维度的特征，商品所在店铺的特征是否对商品的销量有帮助呢？比如在一些商品数比较多的店铺，往往用户搜索进入的概率就大，那商品被用户看到的概率也就变大了。另一方面，店铺的 VIP 用户多寡似乎也能说明店铺是否优质，优质的店铺的商品理论上来说也更好卖一些。</p>
<p data-nodeid="57">综上所述，我们初步总结了五个可能对商品销量有影响的特征。</p>
<p data-nodeid="58">商品维度：</p>
<ul data-nodeid="59">
<li data-nodeid="60">
<p data-nodeid="61">点击数；</p>
</li>
<li data-nodeid="62">
<p data-nodeid="63">添加到购物车的次数；</p>
</li>
<li data-nodeid="64">
<p data-nodeid="65">收藏数。</p>
</li>
</ul>
<p data-nodeid="66">商品所在店铺的维度：</p>
<ul data-nodeid="67">
<li data-nodeid="68">
<p data-nodeid="69">店铺的商品数；</p>
</li>
<li data-nodeid="70">
<p data-nodeid="71">店铺所拥有的 VIP 用户数。</p>
</li>
</ul>
<h4 data-nodeid="72">商品特征计算</h4>
<p data-nodeid="73">首先我们尝试计算商品的特征，从三张表的描述可以得知，在用户行为表中，有用户对商品的行为记录，所以我们从用户行为表入手来计算商品的特征。</p>
<p data-nodeid="74">基本的思路就是把用户行为表按照 item_id 的维度聚合起来，然后点击数、加购物车数、收藏数和购买数都应该是聚合后的表的列。但目前用户行为表中没有这四列，相关的信息都由 action_type 来表示了。</p>
<p data-nodeid="75">所以第一步，我们需要把 action_type 的内容展开成四列来分别表示：点击、加购物车、收藏和下单。</p>
<p data-nodeid="76">规则就是如果 action_type 是收藏时，则收藏列为 1，其他三列为 0；action_type 为下单时，下单一列为 1，其他三列为 0, 以此类推。</p>
<p data-nodeid="77">这里就涉及我们在生成新列的时候，不仅是从现有列直接计算生成，而是需要有一定的逻辑判断（判断 action_type ）的值，所以我们可以用 Series 的 apply 函数来实现，apply 函数可以把一个函数执行到指定的 Series 上，然后把函数的返回值作为新的 Series 返回。</p>
<p data-nodeid="78">代码如下：</p>
<pre class="lang-python" data-nodeid="79"><code data-language="python"><span class="hljs-comment">#&nbsp;分别插入&nbsp;click、cart、order、fav&nbsp;四列，表示点击、加购物车、下单和收藏</span>
<span class="hljs-comment">#&nbsp;规则见上面的文字描述</span>
df_user_log[<span class="hljs-string">"click"</span>]&nbsp;=&nbsp;df_user_log.action_type.apply(<span class="hljs-keyword">lambda</span>&nbsp;l:<span class="hljs-number">1</span>&nbsp;<span class="hljs-keyword">if</span>&nbsp;l==<span class="hljs-number">0</span>&nbsp;<span class="hljs-keyword">else</span>&nbsp;<span class="hljs-number">0</span>)
df_user_log[<span class="hljs-string">"cart"</span>]&nbsp;=&nbsp;df_user_log.action_type.apply(<span class="hljs-keyword">lambda</span>&nbsp;l:<span class="hljs-number">1</span>&nbsp;<span class="hljs-keyword">if</span>&nbsp;l==<span class="hljs-number">1</span>&nbsp;<span class="hljs-keyword">else</span>&nbsp;<span class="hljs-number">0</span>)
df_user_log[<span class="hljs-string">"order"</span>]&nbsp;=&nbsp;df_user_log.action_type.apply(<span class="hljs-keyword">lambda</span>&nbsp;l:<span class="hljs-number">1</span>&nbsp;<span class="hljs-keyword">if</span>&nbsp;l==<span class="hljs-number">2</span>&nbsp;<span class="hljs-keyword">else</span>&nbsp;<span class="hljs-number">0</span>)
df_user_log[<span class="hljs-string">"fav"</span>]&nbsp;=&nbsp;df_user_log.action_type.apply(<span class="hljs-keyword">lambda</span>&nbsp;l:<span class="hljs-number">1</span>&nbsp;<span class="hljs-keyword">if</span>&nbsp;l==<span class="hljs-number">3</span>&nbsp;<span class="hljs-keyword">else</span>&nbsp;<span class="hljs-number">0</span>)
<span class="hljs-comment">#&nbsp;查看添加之后的&nbsp;user_log&nbsp;表</span>
df_user_log
</code></pre>
<p data-nodeid="80">解释一下上面的代码，这里我们用到了 lambda 表达式。lambda 表达式是一种精简的函数表示法。形式如下：</p>
<pre class="lang-python" data-nodeid="81"><code data-language="python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fn</span>(<span class="hljs-params">参数</span>):</span>
  <span class="hljs-keyword">return</span> 参数表达式 
</code></pre>
<p data-nodeid="6376">在 apply 函数中使用 lambda 表达式，Series 会对每个元素都执行一次这个表达式，当前执行的元素就是 lambda 表达式的参数，也就是 l。然后每一次执行，我们都会判断 l 的值（也就是action_type）来判断是返回 0 还是 1。</p>
<p data-nodeid="6377">最后，所有 lambda 表达式的结果汇总起来成一个新的 Series，作为 apply 函数的返回值，然后我们将其赋值给我们创建的新列。</p>

<p data-nodeid="83">上述代码的输出如下，红框部分即为我们添加的列。</p>
<p data-nodeid="7368" class=""><img src="https://s0.lgstatic.com/i/image6/M01/44/B4/Cgp9HWDAdSKAD0I6AADT2CSAToU921.png" alt="Drawing 4.png" data-nodeid="7371"></p>

<p data-nodeid="85">我们能看到 click 这一列都是 1 ，而其他三列都是 0，click 行为本身就远比其他三个多，理论上也说得通，为了避免有 Bug，我们还是查看一下这几列的数据分布。</p>
<pre class="lang-python" data-nodeid="86"><code data-language="python">print(<span class="hljs-string">"click&nbsp;数据分布：\n"</span>,df_user_log.click.value_counts())
print(<span class="hljs-string">"order&nbsp;数据分布：\n"</span>,df_user_log.order.value_counts())
print(<span class="hljs-string">"fav&nbsp;数据分布：\n"</span>,df_user_log.fav.value_counts())
print(<span class="hljs-string">"cart&nbsp;数据分布：\n"</span>,df_user_log.cart.value_counts())
</code></pre>
<p data-nodeid="87">输出为：</p>
<pre class="lang-java" data-nodeid="88"><code data-language="java">click 数据分布：
 <span class="hljs-number">1</span>    <span class="hljs-number">9709458</span>
<span class="hljs-number">0</span>    <span class="hljs-number">1275608</span>
Name: click, dtype: int64
order 数据分布：
 <span class="hljs-number">0</span>    <span class="hljs-number">10325489</span>
<span class="hljs-number">1</span>      <span class="hljs-number">659577</span>
Name: order, dtype: int64
fav 数据分布：
 <span class="hljs-number">0</span>    <span class="hljs-number">10384328</span>
<span class="hljs-number">1</span>      <span class="hljs-number">600738</span>
Name: fav, dtype: int64
cart 数据分布：
 <span class="hljs-number">0</span>    <span class="hljs-number">10969773</span>
<span class="hljs-number">1</span>       <span class="hljs-number">15293</span>
Name: cart, dtype: int64
</code></pre>
<p data-nodeid="89">从上面的数据可以看出，基本还是符合预期的。<br>
之后，我们从原始的行为表中筛选出我们感兴趣的字段。</p>
<pre class="lang-python" data-nodeid="90"><code data-language="python">df_clean&nbsp;=&nbsp;df_user_log[[<span class="hljs-string">"item_id"</span>,&nbsp;<span class="hljs-string">"click"</span>,&nbsp;<span class="hljs-string">"cart"</span>,&nbsp;<span class="hljs-string">"order"</span>,<span class="hljs-string">"fav"</span>]]
df_clean
</code></pre>
<p data-nodeid="8362" class="">输出为：<br>
<img src="https://s0.lgstatic.com/i/image6/M01/44/B5/Cgp9HWDAdTeADUucAAFAbPPN_tM071.png" alt="Drawing 5.png" data-nodeid="8367"></p>

<p data-nodeid="92">筛选了之后，行是没有变化的，但只剩下五列了。接下来，我们将这个表以 item_id 聚合，同一个 item_id 的点击数、下单数、加购物车数和收藏数进行求和。这样我们就可以得到每个商品的汇总结果了。</p>
<pre class="lang-python" data-nodeid="93"><code data-language="python">df_item&nbsp;=&nbsp;df_clean.groupby([<span class="hljs-string">"item_id"</span>]).sum()
df_item
</code></pre>
<p data-nodeid="9362" class="">输出为：<br>
<img src="https://s0.lgstatic.com/i/image6/M01/44/B5/Cgp9HWDAdT-AexVBAADkkVhOqxI168.png" alt="Drawing 6.png" data-nodeid="9367"></p>

<p data-nodeid="95">可以看到，输出的表中 item_id 已经成为索引。并且结果表只剩下 75w 行记录，核心原因就是我们把 item_id 相同的记录聚合了，所以现在 df_item 已经是一个商品维度的特征表。</p>
<p data-nodeid="96">我们以上面的 1113162 号商品为例，数值显示他被点击过 17次，没有被加过购物车，被购买过两次，以及被收藏过一次。</p>
<p data-nodeid="97">至此，我们商品维度的特征就准备完毕了。</p>
<h4 data-nodeid="98">店铺特征计算</h4>
<p data-nodeid="99">店铺特征包含两个，需要分别进行计算。但一致的是，我们都希望得到一个店铺 id，也就是seller_id 作为 index 的表，而特征则是表的列。</p>
<p data-nodeid="100">（1）店铺拥有的 vip 用户数</p>
<p data-nodeid="101">首先我们回顾一下 vip 用户表。</p>
<pre class="lang-python" data-nodeid="102"><code data-language="python">df_vip_user
</code></pre>
<p data-nodeid="10366" class="">输出为：<br>
<img src="https://s0.lgstatic.com/i/image6/M01/44/B5/Cgp9HWDAdUqAUEFnAAE1mAP_a9Q396.png" alt="Drawing 7.png" data-nodeid="10371"></p>

<p data-nodeid="104">表里的 merchant_id 就是商家的 id，但是在用户行为表中，商家的 id 字段为 seller_id。所以为了后续对应方便，我们首先需要将该列重命名为 seller_id。</p>
<p data-nodeid="105">然后我们要计算每一个商家的 VIP 用户数，该表存储的是用户是否是某个商家的 VIP，label 字段为 1 代表是。这里我们只考虑商家维度，所以我们只需要简单按照 seller_id 聚合该表，让 seller_id 一致的记录对应的 label 字段求和，即可得到每个商家的 VIP 用户数。</p>
<pre class="lang-python" data-nodeid="106"><code data-language="python"><span class="hljs-comment">#&nbsp;重命名&nbsp;merchant_id&nbsp;为&nbsp;seller_id</span>
df_vip_user&nbsp;=&nbsp;df_vip_user.rename(&nbsp;columns={<span class="hljs-string">"merchant_id"</span>&nbsp;:&nbsp;<span class="hljs-string">"seller_id"</span>})
<span class="hljs-comment">#&nbsp;选取&nbsp;seller_id和label&nbsp;这连烈，斌将结果通过&nbsp;seller_id&nbsp;聚合，相同&nbsp;seller_id&nbsp;的记录的label求和</span>
df_brand_vip_users&nbsp;=&nbsp;df_vip_user[[<span class="hljs-string">"seller_id"</span>,&nbsp;<span class="hljs-string">"label"</span>]].groupby(<span class="hljs-string">"seller_id"</span>).sum()
<span class="hljs-comment">#&nbsp;查看保存了店铺&nbsp;vip&nbsp;用户数的表的内容</span>
df_brand_vip_users
</code></pre>
<p data-nodeid="11374" class="">输出为：<br>
<img src="https://s0.lgstatic.com/i/image6/M00/44/BD/CioPOWDAdVSAG3YBAACLpFjqkHM366.png" alt="Drawing 8.png" data-nodeid="11379"></p>

<p data-nodeid="108">聚合完毕后，表的索引就会是 seller_id ，label 列则代表 VIP 用户的数量，比如最后一行记录的含义就是 id 为 4993 的店铺，有 4 个 VIP 用户。这样，我们第一个店铺特征表已经准备完毕，表的名字为：df_brand_vip_user。</p>
<p data-nodeid="109">（2）计算每个店铺的商品数</p>
<p data-nodeid="110">计算店铺的商品数，首先我们可以从用户行为表中筛选出 seller_id 和 item_id。 因为我们只关心店铺的商品数，所以我们可以先按 item_id 去重，这样留下来的记录就是商品的总数。然后我们在这个表的基础上，将 item_id 列都赋值为 1 ，然后再按 seller_id 聚合，让 item_id 做求和的运算，就可以得出每个 seller_id 对应的商品数。</p>
<p data-nodeid="111">代码如下：</p>
<pre class="lang-python" data-nodeid="112"><code data-language="python"><span class="hljs-comment">#&nbsp;筛选出&nbsp;seller_id&nbsp;和&nbsp;item_id</span>
df_seller_item_count&nbsp;=&nbsp;df_user_log[[<span class="hljs-string">"seller_id"</span>,&nbsp;<span class="hljs-string">"item_id"</span>]]
<span class="hljs-comment">#&nbsp;按item_id&nbsp;去重，因为&nbsp;itemid&nbsp;一样的记录，seller_id&nbsp;肯定也一样，所以&nbsp;seller_id没有影响</span>
df_seller_item_count&nbsp;=&nbsp;df_seller_item_count.drop_duplicates(<span class="hljs-string">"item_id"</span>)
<span class="hljs-comment">#&nbsp;将&nbsp;item_id&nbsp;列赋值为&nbsp;1，方便做求和</span>
df_seller_item_count[<span class="hljs-string">"item_id"</span>]&nbsp;=&nbsp;<span class="hljs-number">1</span>
<span class="hljs-comment">#&nbsp;按seller_id&nbsp;聚合，然后针对&nbsp;item_id&nbsp;列求和</span>
df_seller_item_count&nbsp;=&nbsp;df_seller_item_count.groupby(<span class="hljs-string">"seller_id"</span>).sum()
<span class="hljs-comment">#&nbsp;将&nbsp;item_id&nbsp;列改为&nbsp;item_count，&nbsp;避免有歧义</span>
df_seller_item_count&nbsp;=&nbsp;df_seller_item_count.rename(columns&nbsp;=&nbsp;{<span class="hljs-string">"item_id"</span>&nbsp;:&nbsp;<span class="hljs-string">"item_count"</span>})
<span class="hljs-comment">#&nbsp;查看结果</span>
df_seller_item_count
</code></pre>
<p data-nodeid="12386" class="">输出为：<br>
<img src="https://s0.lgstatic.com/i/image6/M00/44/BD/CioPOWDAdVyAebgTAACfvacgRqA705.png" alt="Drawing 9.png" data-nodeid="12391"></p>

<p data-nodeid="114">可以看到，我们第二个店铺特征的表也准备好了，index 为 seller_id， 有一列 item_count 代表店铺的商品总数。</p>
<h4 data-nodeid="115">商品-店铺特征关联</h4>
<p data-nodeid="116">接下来，我们就把两个店铺特征表拼接到我们的商品特征表中。</p>
<p data-nodeid="117">先来看一下商品特征表。</p>
<pre class="lang-python" data-nodeid="118"><code data-language="python">df_item
</code></pre>
<p data-nodeid="13402" class="">输出为：<br>
<img src="https://s0.lgstatic.com/i/image6/M00/44/B5/Cgp9HWDAdWKAfB8oAADpCIEDDBU354.png" alt="Drawing 10.png" data-nodeid="13407"></p>

<p data-nodeid="120">可以看到，目前商品特征表中只有 item_id，没有 seller_id。 所以没有办法直接关联店铺特征表。所以第一步，我们首先需要把商品特征表增加 seller_id 的字段。</p>
<p data-nodeid="121">要增加 seller_id，只需要我们用类似之前的方法，从原始的行为表中抽取出 seller_id 和 item_id 的对应关系表，然后再将对应关系表拼接进商品特征表中即可。代码如下：</p>
<pre class="lang-python" data-nodeid="122"><code data-language="python"><span class="hljs-comment">#&nbsp;从原始行为表中取出&nbsp;item_id&nbsp;和seller_id&nbsp;，构成新表</span>
df_brand_item_map&nbsp;=&nbsp;df_user_log[[<span class="hljs-string">"item_id"</span>,&nbsp;<span class="hljs-string">"seller_id"</span>]]
<span class="hljs-comment">#&nbsp;按照item_id去重，去重后得到的结果就相当于是&nbsp;item&nbsp;id&nbsp;和&nbsp;seller_id&nbsp;的映射关系</span>
df_brand_item_map&nbsp;=&nbsp;df_brand_item_map.drop_duplicates(<span class="hljs-string">"item_id"</span>)
<span class="hljs-comment">#&nbsp;将&nbsp;df_brand_item_map&nbsp;映射进&nbsp;df_item，&nbsp;以&nbsp;item_id&nbsp;为&nbsp;key</span>
df_item&nbsp;=&nbsp;df_item.merge(df_brand_item_map,&nbsp;how=<span class="hljs-string">"left"</span>,&nbsp;on=<span class="hljs-string">"item_id"</span>)
<span class="hljs-comment">#&nbsp;查看最新的商品特征表</span>
df_item
</code></pre>
<p data-nodeid="14422" class="">输出：<br>
<img src="https://s0.lgstatic.com/i/image6/M00/44/B5/Cgp9HWDAdWyAYZw6AAD16QpZvtY187.png" alt="Drawing 11.png" data-nodeid="14427"></p>

<p data-nodeid="124">可以看到，现在我们商品特征表已经多了 seller_id 的字段，现在我们可以将两个店铺特征表拼接进商品特征表了。</p>
<p data-nodeid="125">首先拼接店铺的 VIP 用户数表。代码如下：</p>
<pre class="lang-python" data-nodeid="126"><code data-language="python"><span class="hljs-comment">#&nbsp;将店铺vip用户特征表拼接到商品特征表中，以&nbsp;seller_id&nbsp;为&nbsp;key</span>
df_item&nbsp;=&nbsp;df_item.merge(df_brand_vip_users,how&nbsp;=&nbsp;<span class="hljs-string">"left"</span>,&nbsp;on=<span class="hljs-string">"seller_id"</span>)
df_item
</code></pre>
<p data-nodeid="15446" class="">输出：<br>
<img src="https://s0.lgstatic.com/i/image6/M00/44/B5/Cgp9HWDAdXOAFSiHAAD8qQyG2l4993.png" alt="Drawing 12.png" data-nodeid="15451"></p>

<p data-nodeid="128">从输出的来看，店铺 VIP 会员数已经拼接到表里了，但是出现了空值，这也是符合预期的。毕竟 VIP 用户表里不是每个店铺都有 VIP 用户。为了不影响后续的模型，我们通过之前学习的填补缺失值的方法，给空值填为 0，这也是符合逻辑的。</p>
<pre class="lang-python" data-nodeid="129"><code data-language="python"><span class="hljs-comment">#&nbsp;用&nbsp;0&nbsp;填充缺失值</span>
df_item[<span class="hljs-string">"label"</span>]&nbsp;=&nbsp;df_item[<span class="hljs-string">"label"</span>].fillna(<span class="hljs-number">0</span>)
df_item
</code></pre>
<p data-nodeid="16474" class="">输出：<br>
<img src="https://s0.lgstatic.com/i/image6/M00/44/BE/CioPOWDAdXmAP-xYAAD1e2V4Jo0896.png" alt="Drawing 13.png" data-nodeid="16479"></p>

<p data-nodeid="131">可以看到，通过 fillna 之后，缺失值被替换为了0。</p>
<p data-nodeid="132">下一步，我们来拼接店铺的商品数特征表。代码如下：</p>
<pre class="lang-python" data-nodeid="133"><code data-language="python">df_item&nbsp;=&nbsp;df_item.merge(df_seller_item_count,&nbsp;how&nbsp;=&nbsp;<span class="hljs-string">"left"</span>,&nbsp;on&nbsp;=&nbsp;<span class="hljs-string">"seller_id"</span>)
df_item
</code></pre>
<p data-nodeid="17506" class="">输出：<br>
<img src="https://s0.lgstatic.com/i/image6/M00/44/BE/CioPOWDAdX-AduUrAADJaIVZlp0877.png" alt="Drawing 14.png" data-nodeid="17511"></p>

<p data-nodeid="135">现在，我们的商品特征表已经完成了，不仅有商品的点击、加购物车、下单和收藏记录，还有商品所在店铺的 VIP 用户数和店铺的商品数。下一步我们就基于这张表建立模型进行分析。</p>
<h3 data-nodeid="136">回归分析</h3>
<p data-nodeid="137">激动人心的时刻马上就要到了，现在我们通过特征工程的环节，已经整理好了能够预测商品销量的特征表，现在我们建立线性回归模型来进行分析。</p>
<h4 data-nodeid="138">拆分训练、测试集合</h4>
<p data-nodeid="139">在训练之前，我们还需要想一个问题，训练出模型后，我们怎么衡量模型的好坏呢？总不能等到真正要在生产环境用上模型的时候才进行测试。</p>
<p data-nodeid="140">一般常见的做法是，将现有的数据拆成两份，比如 70% 一份，30% 一份。 70% 的数据用来训练模型，训练完之后用剩下的 30% 的数据进行测试。这样就能够在模型上线之前就能够衡量模型的好坏。sklearn 提供了现成的 train_test_split 函数，可以帮我们实现数据集的分割。</p>
<p data-nodeid="141">首先第一步，我们要从商品特征表中拆开自变量和因变量。</p>
<ul data-nodeid="142">
<li data-nodeid="143">
<p data-nodeid="144">因变量是很明显的，就是 order 列，代表商品被下单的数量。</p>
</li>
<li data-nodeid="145">
<p data-nodeid="146">自变量就是除了 order 列之外的所有数值特征，也就是从商品特征表删掉 item_id，seller_id 和 order 列，剩下的都可以作为自变量。</p>
</li>
</ul>
<p data-nodeid="147">代码如下：</p>
<pre class="lang-python" data-nodeid="148"><code data-language="python"><span class="hljs-comment">#&nbsp;导入分割的方法</span>
<span class="hljs-keyword">from</span>&nbsp;sklearn.model_selection&nbsp;<span class="hljs-keyword">import</span>&nbsp;train_test_split
<span class="hljs-comment">#&nbsp;自变量的数据表</span>
X&nbsp;=&nbsp;df_item.drop(columns=[<span class="hljs-string">"order"</span>,&nbsp;<span class="hljs-string">"seller_id"</span>,<span class="hljs-string">"item_id"</span>])
<span class="hljs-comment">#&nbsp;因变量</span>
y&nbsp;=&nbsp;df_item[<span class="hljs-string">"order"</span>]
<span class="hljs-comment">#&nbsp;分别切割出训练集，测试集，测试机的比例是&nbsp;20%</span>
X_train,&nbsp;X_test,&nbsp;y_train,&nbsp;y_test&nbsp;=&nbsp;train_test_split(X,&nbsp;y,&nbsp;test_size&nbsp;=&nbsp;<span class="hljs-number">0.20</span>,&nbsp;random_state=<span class="hljs-number">42</span>)
<span class="hljs-comment">#&nbsp;查看自变量的测试集合</span>
X_train
</code></pre>
<p data-nodeid="149">输出：</p>
<p data-nodeid="18542" class="te-preview-highlight"><img src="https://s0.lgstatic.com/i/image6/M00/44/BE/CioPOWDAdYeATsL7AAE8k-NpbQ8829.png" alt="Drawing 15.png" data-nodeid="18545"></p>

<p data-nodeid="151">我们商品特征表本身有 75w 行。这里打印出的训练集有 60w 行，刚好占比 80%。可以看到 train_test_split 函数干得还不错。</p>
<h4 data-nodeid="152">建立线性回归模型</h4>
<p data-nodeid="153">接下来的步骤就比较简单，我们建立回归模型，然后调用 fit 方法来从 X_train 和 y_train 中训练模型。</p>
<pre class="lang-python" data-nodeid="154"><code data-language="python">linear_model&nbsp;=&nbsp;LinearRegression()
linear_model.fit(X_train,&nbsp;y_train)
</code></pre>
<p data-nodeid="155">训练结束之后，我们可以用 linear_model 来 predict X_test，然后拿 predict 函数返回的结果和 y_test 比较，就能知道我们模型的误差。</p>
<pre class="lang-python" data-nodeid="156"><code data-language="python"><span class="hljs-comment">#&nbsp;predict&nbsp;X_test&nbsp;对应的&nbsp;y&nbsp;值</span>
y_pred&nbsp;=&nbsp;linear_model.predict(X_test)
<span class="hljs-comment">#&nbsp;用&nbsp;scores&nbsp;方法，查看模型自变量和因变量的相关性</span>
print(<span class="hljs-string">"Scores:"</span>,&nbsp;linear_model.score(X_test,&nbsp;y_test))
<span class="hljs-comment">#&nbsp;查看&nbsp;predict&nbsp;y&nbsp;和&nbsp;y&nbsp;test的平均绝对误差print("MAE:",&nbsp;mean_absolute_error(y_test,&nbsp;y_pred))</span>
<span class="hljs-comment">#&nbsp;查看模型的&nbsp;b&nbsp;值</span>
print(<span class="hljs-string">"intercept:"</span>,&nbsp;linear_model.intercept_)
<span class="hljs-comment">#&nbsp;查看模型的系数</span>
print(<span class="hljs-string">"coef_:"</span>,&nbsp;linear_model.coef_)
</code></pre>
<p data-nodeid="157">输出：</p>
<pre class="lang-java" data-nodeid="158"><code data-language="java">Scores: <span class="hljs-number">0.4417000916369106</span>
MAE: <span class="hljs-number">0.9939945446528016</span>
intercept: <span class="hljs-number">0.32490105754921395</span>
coef_: [-<span class="hljs-number">1.09570715e-03</span>  <span class="hljs-number">7.64421017e+00</span>  <span class="hljs-number">7.20958736e-01</span>  <span class="hljs-number">6.07684870e-04</span>
 -<span class="hljs-number">3.10573289e-04</span>]
</code></pre>
<p data-nodeid="159">模型的相关性分数说 0.44， 对于现实世界中的回归问题来说，这也算是个不错的成绩，说明我们的特征还是很大程度能够影响下单量。MAE 是 0.99，说明对于测试集而言，我们模型预测的结果和实际的真实结果非常接近，说明模型的拟合还是比较好的。</p>
<p data-nodeid="160">从模型系数中可以看到，我们的前三个特征：click、fav、cart 的系数比较大，后两个特征 label 和 item_count 的系数比较小，说明对于商品的下单量而言，前三个特征更加重要，关联性更强，后两者则对结果影响相对较小。</p>
<p data-nodeid="161">有时候凭我们的主观判断，对于特征的重要性判断可能是不准的，我们可以都一并带上，然后在模型训练的环节，通过拟合算法自动去找到不同特征的重要性。这样就比人的判断靠谱多了。</p>
<p data-nodeid="162">我们来测试一下我们的模型，假设某个商品在预热期间，一共有 100 次点击，2 次加购物车，6 次收藏，这个商品所在的店铺有 10 个vip 用户，一共有 30 个商品。那根据我们的模型来预测这个商品的下单数：</p>
<pre class="lang-python" data-nodeid="163"><code data-language="python">print(linear_model.predict([[<span class="hljs-number">100</span>,&nbsp;<span class="hljs-number">2</span>&nbsp;,<span class="hljs-number">6</span>,&nbsp;<span class="hljs-number">10</span>,&nbsp;<span class="hljs-number">30</span>]]))
</code></pre>
<p data-nodeid="164">输出：</p>
<pre class="lang-java" data-nodeid="165"><code data-language="java">[<span class="hljs-number">19.82626275</span>]
</code></pre>
<p data-nodeid="166">代表可能会被买 19 次。</p>
<h3 data-nodeid="167">小结</h3>
<p data-nodeid="168">至此，我们今天的实战就学习完毕了，总结一下，今天我们主要学习了：</p>
<ul data-nodeid="169">
<li data-nodeid="170">
<p data-nodeid="171">回归类型任务的分析思路与方法；</p>
</li>
<li data-nodeid="172">
<p data-nodeid="173">常见的特征工程的思路与方法；</p>
</li>
<li data-nodeid="174">
<p data-nodeid="175">通过 df.groupby("xxx").sum() 的形式来聚合数据表；</p>
</li>
<li data-nodeid="176">
<p data-nodeid="177">通过 df.merge 函数来拼接数据表，前提是两个数据表有相同的字段；</p>
</li>
<li data-nodeid="178">
<p data-nodeid="179">通过 train_test_split 来将数据集拆分为训练集和测试集；</p>
</li>
<li data-nodeid="180">
<p data-nodeid="181">通过 LinearRegression 来建立线性回归模型。</p>
</li>
</ul>
<p data-nodeid="182">通过本节的学习，相信你对回归预测类型的分析能够初步找到初步感觉。当然这一块是非常大的领域，挑战也很多。本系列课程重点还是以带大家入门为主，有兴趣的同学可以自行深入研究。</p>
<p data-nodeid="183">课后习题：</p>
<p data-nodeid="184">尝试调整用于训练的特征，看看是否能够进一步降低 MAE。</p>
<hr data-nodeid="185">
<p data-nodeid="186">答案（其中一种）：</p>
<p data-nodeid="187">既然从结果系数来看，label 和 item_count 对结果的贡献较小，可以从训练特征表中删除，这样训练出的模型的 MAE 是 0.95。</p>

---

### 精选评论


